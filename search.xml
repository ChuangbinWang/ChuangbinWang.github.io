<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[elasticsearch底层存储原理]]></title>
    <url>%2F2019%2F09%2F23%2Felasticsearch%E5%BA%95%E5%B1%82%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Elasticsearch 是一个建立在全文搜索引擎 Apache Lucene(TM) 基础上的搜索引擎，他能将每一个字段都编入索引，使其可以被搜索。本文的核心是探讨其作为搜索引擎的内部核心存储机制。本文总结自https://neway6655.github.io/elasticsearch/2015/09/11/elasticsearch-study-notes.htmlhttps://www.infoq.cn/article/database-timestamp-02/?utm_source=infoq&amp;utm_medium=related_content_link&amp;utm_campaign=relatedContent_articles_clkElasticsearch是面向文档型数据库，一条数据在这里就是一个文档，该文档属于一个User的类型，各种各样的类型会属于一个索引。其中用JSON作为文档序列化的格式，比如下面这条用户数据：{“name” : “John”,“sex” : “Male”,“age” : 25,“birthDate”: “1990/05/01”,“about” : “I love to go rock climbing”,“interests”: [ “sports”, “music” ]}Elasticsearch和关系型数据术语对照表:关系数据库 ⇒ 数据库 ⇒ 表 ⇒ 行 ⇒ 列(Columns)Elasticsearch ⇒ 索引 ⇒ 类型 ⇒ 文档 ⇒ 字段(Fields)Elasticsearch最关键的就是提供强大且丰富的索引功能，它的索引思路，就是将磁盘里的东西尽量搬进内存，减少磁盘随机读取次数(同时也利用磁盘顺序读特性)，结合各种奇妙的压缩算法，用及其苛刻的态度使用内存。当然了在提高搜索的性能的同时，难免会牺牲某些其他方面的性能，比如插入/更新，因为在插入数据的同时它还会为每个字段建立倒排索引。在传统的数据库中，由于二叉树的查找效率是logN，插入和删除的效率高，且在树的结构下插入新的节点不会移动全部节点，又结合于磁盘读取特性，传统的数据库使用B-Tree/B+-Tree。为了提高查询的效率，减少磁盘寻道次数，将多个值作为一个数组通过连续区间存放，一次寻道读取多个数据，同时也降低树的高度。建立倒排索引的流程：示例流程：得到的倒排索引如下:Posting ListElasticsearch分别为每个field都建立了一个倒排索引，Kate, John, 24, Female这些叫term（term的定义），而[1,2]就是Posting List。Posting list就是一个int的数组，存储了所有符合某个term的文档id。通过posting list这种索引方式似乎可以很快进行查找，比如要找age=24的同学，爱回答问题的小明马上就举手回答：我知道，id是1，2的同学。但是，如果这里有上千万的记录呢？如果是想通过name来查找呢？在传统的数据库中使用B-Tree减少磁盘寻道次数，直接将索引用一个b-tree树连接在一起并存入一个数据页中，而elasticsearch也使用同样的方法，它首先将term进行排序，并用字典树的方法将其连接起来形成term index。而建立这些term和文档id 的关系就需要前面两步，term index，和建立term dictionaryTerm index这棵树不会包含所有的term，它只包含的是term的一些前缀。主要是由于如果term太多，term index也会很大，放内存不现实。在mysql中是把索引存入硬盘，然后需要若干次读取磁盘进行数据寻找。而该字典树又像B-tree一样会存储该节点内的具体信息，类似，它的每个节点会存储该term index下的term dictionary blocks信息，而每个term dictionary block又会执行其对应的Posting List信息。再结合FST(Finite State Transducers)的压缩技术，可以使term index缓存到内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘随机读的次数。那FST压缩算法是什么。它用在什么地方呢？FSTs are finite-state machines that map a term (byte sequence) to an arbitrary output.FSTs是有限状态机，它将术语(字节序列)映射到任意输出。画个图你就知道。假设我们现在要将mop, moth, pop, star, stop and top(term index里的term前缀)映射到序号：0，1，2，3，4，5(term dictionary的block位置)。最简单的做法就是定义个Map&lt;String, Integer&gt;，然后将对应数据存入map中，但是这样太消耗内存。而FST就是将单词分成单个字母通过⭕️和–&gt;表示出来，0权重不显示。如果⭕️后面出现分支，就标记权重，最后整条路径上的权重加起来就是这个单词对应的序号。⭕️ 表示一种状态–&gt;表示状态的变化过程，上面的字母/数字表示状态变化和权重FST就用于term index字典树和term index指向term dictionary blocks时 term dictionary blocks的存储。ES还对posting list 做了压缩，其压缩方法是：Frame Of Reference， Roaring bitmapsFrame Of Reference：原理就是通过增量，将原来的大数变成小数仅存储增量值，再精打细算按bit排好队，最后通过字节存储，而不是大大咧咧的尽管是2也是用int(4个字节)来存储。这个 Frame of Reference 的编码是有解压缩成本的。当进行查找的时候可以利用 skip list，除了跳过了遍历的成本，也跳过了解压缩这些压缩过的 block 的过程，从而节省了 cpu。Roaring bitmaps传统的Bitmap是一种数据结构，假设有某个posting list：[1,3,4,7,10]对应的bitmap就是：[1,0,1,1,0,0,1,0,0,1]虽然这个方法已经相对好，但还不算很压缩率高，且其缺点是存储空间随着 变量个数线性增长。而Roaring bitmaps，将posting list按照65535为界限分块，比如第一块所包含的文档id范围在065535之间，第二块的id范围是65536131071，以此类推。再用&lt;商，余数&gt;的组合表示每一组id，这样每组里的id范围都在0~65535内了，剩下的就好办了，既然每组id不会变得无限大，那么我们就可以通过最有效的方式对这里的id存储。可是为什么是以65535为界限?65535也是一个经典值，因为它=2^16-1，正好是用2个字节能表示的最大数（选用2个字节的原因是大部分情况，设置一个字节那最多表示256-1 个数，区间的长度太小，而 三个字节又比较难用short或者char表示，选用4个组件那区间的空间太大不合适），一个short的存储单位，注意到上图里的最后一行“If a block has more than 4096 values（如果这个块里面有超过4096个值时）, encode as a bit set, and otherwise as a simple array using 2 bytes per value”，如果是大块，用节省点用bitset存，小块就豪爽点，2个字节我也不计较了，用一个short[]存着方便。那为什么用4096来区分采用数组还是bitmap的阀值呢？这个是从内存大小考虑的，当block块里元素超过4096后，用bitmap更剩空间：采用bitmap需要的空间是恒定的: 65536/8 = 8192bytes而如果采用short[]，所需的空间是: 2*N(N为数组元素个数)更通俗的讲法是由于选用2个字节表示一个最大的数那么其占有的空间 65536/8 = 8192个字节，而如果是小块，且数值最大也是65535，用short类型来表示这个最大数值也只需要2个字节，那么8192个字节，就只能表示4096个数。如何减少文档数？Elasticsearch 有一个功能可以实现类似的优化效果，那就是 Nested Document。我们可以把一段时间的很多个数据点打包存储到一个父文档里，变成其嵌套的子文档。示例如下：{timestamp:12:05:01, idc:sz, value1:10,value2:11}{timestamp:12:05:02, idc:sz, value1:9,value2:9}{timestamp:12:05:02, idc:sz, value1:18,value:17}可以打包成：{max_timestamp:12:05:02, min_timestamp: 1205:01, idc:sz,records: [{timestamp:12:05:01, value1:10,value2:11}{timestamp:12:05:02, value1:9,value2:9}{timestamp:12:05:02, value1:18,value:17}]}这样可以把数据点公共的维度字段上移到父文档里，而不用在每个子文档里重复存储，从而减少索引的尺寸。这样** 使用了嵌套文档之后，对于 term 的 posting list 只需要保存父文档的 doc id 就可以了，可以比保存所有的数据点的 doc id 要少很多。如果我们可以在一个父文档里塞入 50 个嵌套文档，那么 posting list 可以变成之前的 1/50。**联合索引：上面说了半天都是单field索引，如果多个field索引的联合查询，倒排索引如何满足快速查询的要求呢？在posting list中利用跳表(Skip list)的数据结构快速做“与”运算，或者利用上面提到的bitset按位“与”先看看跳表的数据结构：将一个有序链表level0，挑出其中几个元素到level1及level2，每个level越往上，选出来的指针元素越少，查找时依次从高level往低查找，比如55，先找到level2的31，再找到level1的47，最后找到55，一共3次查找，查找效率和2叉树的效率相当，但也是用了一定的空间冗余来换取的。但是在posting list中如果进行联合索引时，由于已经建立好不同长度链表，将其转换为跳表结构体后，最短的posting list中的每个id，逐个在另外两个posting list中查找看是否存在，最后得到交集的结果。当posting list 中的blocking 长度大于4096时，其存储形式是 bitset，那么，直接按位与，得到的结果就是最后的交集。最后，总结：对于使用Elasticsearch进行索引时需要注意:不需要索引的字段，一定要明确定义出来，因为默认是自动建索引的同样的道理，对于String类型的字段，不需要analysis的也需要明确定义出来，因为默认也是会analysis的选择有规律的ID很重要，随机性太大的ID(比如java的UUID)不利于查询]]></content>
      <categories>
        <category>-elasticsearch -数据库</category>
      </categories>
      <tags>
        <tag>-底层原理 -elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webrtc实现流程及原理探索]]></title>
    <url>%2F2019%2F09%2F20%2Fwebrtc%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B%E5%8F%8A%E5%8E%9F%E7%90%86%E6%8E%A2%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[目前实时流媒体主流有三种实现方式：WebRTC、HLS、RTMP HLS:是有苹果开发的 一种把流媒体拆分成多个独立小文件的技术，按照播放时间请求不同文件，把hls的文件进行解复用之后取出音视频数据然后丢给video去播放（Safari和安卓版的Chrome能直接播放hls）。 它的优点是：使用了传统http协议，所以兼容性和稳定都非常好，服务端可以把hls文件上传到cdn，进而能够应对百万级别观众的直播，缺点是延时比较大，通常在10s以上，适合观众和主播没有什么交互的场景。因为一个hls文件时间长度通常在10s以上，再加上生成文件的时间就导致延迟很大。 RTMP：是Adobe推出的，使用长连接，是一套完整的流媒体传输协议，使用flv视频容器，原生浏览器不支持（flash插件支持），不过可以使用websocket + MSE的方式，相关的类库比较少，在Android/IOS客户端上的直播应该用得比较多一点。相对于HLS请求分片的形式，RTMP由于使用长连接，接收不间断的数据流，它的延迟要比HLS小很多，通常是1~3秒，所以如果观众和主播之间有通话或者视频交互，这种方式的延迟是可以接受的。 WebRTC：所有主流浏览器都支持，做到比RTMP提供更低的延迟和更小的缓冲率，官方还提供了配套的native的Andorid/IOS的库， 不过实际的实现可能是套一个webview，由webview启动webrtc，再把数据给native层渲染。（1）getUserMedia是负责获取用户本地的多媒体数据，如调起摄像头录像等。（2）RTCPeerConnection是负责建立P2P连接以及传输多媒体数据。（3）RTCDataChannel是提供的一个信令通道，在游戏里面信令是实现互动的重要元素。 穿墙打洞要建立一个连接需要知道对方的IP地址和端口号，在局域网里面一台路由器可能会连接着很多台设备，例如家庭路由器接入宽带的时候宽带服务商会分配一个公网的IP地址，所有连到这个路由器的设备都共用这个公网IP地址。如果两台设备都用了同一个端口号创建套接字去连接服务，这个时候就会冲突，因为对外的IP是一样的。因此路由器需要重写IP地址/端口号进行区分，如下图所示：有两台设备分别用了相同的端口号建立连接，被路由器转换成不同的端口，对外网表现为相同IP地址不同端口号，当服务器给这两个端口号发送数据的时候，路由器再根据地址转换映射表把数据转发给相应的主机。所以当你在本地监听端口号为55020，但是对外的端口号并不是这个，对方用55020这个端口号是连不到你的。这个时候有两种解决方法，第一种是在路由器设置一下端口映射，如下图所示：上图的配置是把所有发往8123端口的数据包到转到192.168.123.20这台设备上。但是我们不能要求每个用户都这么配他们的路由器，因此就有了穿墙打洞，基本方法是先由服务器与其中一方（Peer）建立连接，这个时候路由器就会建立一个端口号内网和外网的映射关系并保存起来，如上面的外网1091就可以打到电脑的55020的应用上，这样就打了一个洞，这个时候服务器把1091端口加上IP地址告诉另一方（Peer），让它用这个打好洞的地址进行连接。这就是建立P2P连接穿墙打洞的原理，最早起源于网络游戏，因为打网络游戏经常要组网，WebRTC对NAT打洞进行了标准化。 这个的有效性受制于用户的网络拓扑结构，因为如果路由器的映射关系既取决于内网的IP + 端口号，也取决于服务器的IP加端口号，这个时候就打不了洞了，因为服务器打的那个洞不能给另外一个外网的应用程序使用（会建立不同的映射关系）。相反如果地址映射表只取决于内网机器的IP和端口号那么是可行的。打不了洞的情况下WebRTC也提供了解决方法，即用一个服务器转发多媒体数据。 这套打洞的机制叫ICE（Interactive Connectivity Establishment），帮忙打洞的服务器叫TURN服务，转发多媒体数据的服务器叫STUN服务。谷歌提供了一个turn server（https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice/） 除了默认提供的TURN服务打洞之外，还需要有一个websocket服务交换互连双方的信息。首先打开摄像头获取到本地的mediaStream，并把它添加到RTCPeerConnection的对象里面，然后创建一个本地的offer，这个offer主要是描述本机的一些网络和媒体信息，采用SDP（ Session Description Protocol）格式。然后把这个offer通过websocket服务发送给要连接的对方，对方收到后创建一个answer，格式、作用和offer一样，发送给呼叫方告知被呼叫方的一些信息。当任意一方收到对方的sdp信息后就会调setRemoteDescription记录起来。而当收到默认的ice server发来的打洞信息candidate之后，把candidate发送给对方（在setRemoteDesc之后），让对方发起连接，成功的话就会触发onaddstream事件，把事件里的event.stream画到video上面即可得到对方的影像。 以上就是建立用webrtc建立p2p通信的过程。 具体的代码执行流程如下 其中需要注意的实现由：webrtc 开发之前必须了解的东西1、创建offer的时候带上参数：{ offerToReceiveAudio: true, offerToReceiveVideo: true }2、onicecandidate 必须写在 setLocalDescription 之前，因为一调用setLocalDescription，立马会产生icecandidate。3、pc.addTrack（或者addStream）必须在pc.createAnswer之前，如果你的offer没有带上参数（第一条），那么也应该在pc.createOffer()之前。因为offer或者answer带有媒体信息。4、webrtc 是 peer to peer ，不是peers to peers。A与B 相连，A需要new RTCPeerConnection，B也需要。A再与C相连，A还需要new RTCPeerConnection5、stun 服务器，是提供打洞的东西，turn服务器是提供数据中转的东西。打洞（Nat类型及穿透）请看我的另一篇博客，webrtc只支持 https和 localhost，所以局内网主机能开视频，另一端（另一台电脑）只能看。6、火狐报错：xxx failed（需要turn但根本没有turn服务器），还有一种，turn Server appears to be broken：这个是turn服务器限制连接数量，就是说人较多，你挤不上去了。免费的turn 是http://numb.viagenie.ca，去申请账号密码。免费的stun : stun:stun.freeswitch.org 、stun.ekiga.net自己部turn: github 搜 coturn个人网页：gusheng123.top，可测试多对多视频。 其中废弃的api有navigator.getUserMeida(已废弃)，现在改为navigator.mediaDevices.getUserMedia;RTCPeerConnection.addStream被RTCPeerConnection.addTrack取代;STUN,TURN配置里的url现被urls取代； webrtc的api文档https://www.kaifaxueyuan.com/frontend/webrtc/webrtc-rtcpeerconnection-apis.html sdp消息分享后，在进行连接之前，会进行候选者检索。本机候选者 进行webrtc的端口转换的协议叫做STUN协议，用于收集，srflx (可用于p2p)的候选者TURN服务器，又叫中继服务器，用于收集中继类型的 webrtc候选者。李超教程p2p直播间流程图 当调用createOffer和createAnswer时就会自己触发 onicecandidate 事件]]></content>
      <categories>
        <category>-webrtc -音视频 -实战</category>
      </categories>
      <tags>
        <tag>-webrtc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RTP,RTMP,RTCP,RTSP的区别]]></title>
    <url>%2F2019%2F09%2F20%2FRTP-RTMP-RTCP-RTSP%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[RTP(Real-time Transport Protocol)实时传输协议RTP是用于Internet上针对多媒体数据流的一种传输协议。位于应用层。RTP由两个部分组成:RTP—-传送具有实时属性的数据；RTCP控制协议（RTCP）—-监控服务质量并传送正在进行的会话参与者的相关信息。RTP协议是建立在UDP协议上的。RTP协议详细说明了在互联网上传递音频和视频的标准数据包格式。RTP协议的目的是提供实时数据（如交互式的音频和视频）的端到端传输服务，因此在RTP中没有连接的概念，它可以建立在底层的面向连接或面向非连接的传输协议之上；RTP也不依赖于特别的网络地址格式，底层传输协议只需要支持组帧（Framing）和分段（Segmentation）就足够了；另外RTP本身还不提供任何可靠性机制，这些都要由传输协议或者应用程序自己来保证。在典型的应用场合下，RTP一般是在传输协议之上作为应用程序的一部分加以实现的。RTP并不保证传送或防止无序传送，也不确定底层网络的可靠性。RTP实行有序传送，RTP中的序列号允许接收方重组发送方的包序列，同时序列号也能用于决定适当的包位置，例如：在视频解码中，就不需要顺序解码。 RTCP(Real-time Transport Control Protocol)实时传输控制协议介绍：RTCP控制协议需要与RTP数据协议一起配合使用，当应用程序启动一个RTP会话时将同时占用两个端口，分别供RTP和RTCP使用。RTP本身并不能为按序传输数据包提供可靠的保证，也不提供流量控制和拥塞控制，这些都由RTCP来负责完成。RTCP为RTP媒体流提供信道外控制。功能：通常RTCP会采用与RTP相同的分发机制，向会话中的所有成员周期性地发送控制信息，应用程序通过接收这些数据，从中获取会话参与者的相关资料，例如：传输字节数，传输分组数，丢失分组数，时延抖动，单向和双向网络延迟等等。，从而能够对服务质量进行控制或者对网络状况进行诊断。RTCP收集相关媒体连接的统计信息，网络应用程序可以利用RTCP所提供的信息试图提高服务质量，比如限制信息流量或改用压缩比较小的编解码器。RTCP本身不提供数据加密或身份认证，其伴生协议SRTCP（安全实时传输控制协议）则可用于此类用途。 RTSP（Real Time Streaming Protocol）实时流协议RTSP是由Real Network和Netscape共同提出的如何有效地在IP网络上传输流媒体数据的应用层协议。RTSP对流媒体提供了诸如暂停，快进等控制，但它本身并不传输数据，RTSP的作用相当于流媒体服务器的远程控制。服务器端可以自行选择使用TCP或UDP或组播UDP或RTP等通道来发送数据，具有很好的扩展性。|它的语法和运作跟HTTP 1.1类似，RTSP协也议定义了一对多应用程序如何有效通过IP网络传送多媒体数据。RTSP在体系结构上位于RTP和RTCP之上。HTTP与RTSP相比，HTTP传送HTML，而RTP传送的是多媒体数据。HTTP请求由客户机发出，服务器做出响应；RTSP可以是双向的，即客户机和服务器都可以发出请求。不特别强调时间同步，所以比较能容忍网络延迟。RTSP之所以特意使用与HTTP/1.1类似的语法和操作，在很大程度上是为了兼容现有的Web基础结构，正因如此，HTTP/1.1的扩展机制大都可以直接引入到RTSP中。由RTSP控制的媒体流集合可以用表示描述（Presentation Description）来定义，所谓表示是指流媒体服务器提供给客户机的一个或者多个媒体流的集合，而表示描述则包含了一个表示中各个媒体流的相关信息，如数据编码/解码算法、网络地址、媒体流的内容等。虽然RTSP服务器同样也使用标识符来区别每一流连接会话（Session），但RTSP连接并没有被绑定到传输层连接（如TCP等），也就是说在整个 RTSP连接期间，RTSP用户可打开或者关闭多个对RTSP服务器的可靠传输连接以发出RTSP 请求。此外，RTSP连接也可以基于面向无连接的传输协议（如UDP等）。（2）RTSP协议是共有协议，并有专门机构做维护。.（3）RTSP协议一般传输的是 ts、mp4 格式的流。（4）RTSP传输一般需要 2-3 个通道，命令和数据通道分离。 RTMP(Real Time Messaging Protocol)实时消息传输协议RTMP（Real Time Messaging Protocol）是Adobe Systems公司为Flash播放器和服务器之间音频、视频和数据传输开发的开放协议。它有三种变种：（1）工作在TCP之上的明文协议，使用端口1935；（2）RTMPT封装在HTTP请求之中，可穿越防火墙；（3）RTMPS类似RTMPT，但使用的是HTTPS连接。（4）RTMP协议一般传输的是 flv，f4v 格式流。（5）RTMP一般在 TCP 1个通道上传输命令和数据RTMP视频播放的特点：（1）RTMP协议是采用实时的流式传输，所以不会缓存文件到客户端，这种特性说明用户想下载RTMP协议下的视频是比较难的；（2）视频流可以随便拖动，既可以从任意时间点向服务器发送请求进行播放，并不需要视频有关键帧。相比而言，HTTP协议下视频需要有关键帧才可以随意拖动。（3）RTMP协议支持点播/回放（通俗点将就是支持把flv,f4v,mp4文件放在RTMP服务器，客户端可以直接播放），直播（边录制视频边播放）。 HLS (HTTP Live Streaming)HTTP Live Streaming(HLS)是苹果公司实现的基于HTTP的流媒体传输协议，可实现流媒体的直播和点播，主要应用于iOS系统。HLS点播是分段HTTP点播，不同在于它的分段非常小。要实现HLS点播，重点在于对媒体文件分段，目前有不少开源工具可以使用。相对于常见的流媒体直播协议，HLS直播最大的不同在于，直播客户端获取到的并不是一个完整的数据流，HLS协议在服务器端将直播数据流存储为连续的、很短时长的媒体文件（MPEG-TS格式），而客户端则不断的下载并播放这些小文件，因为服务器总是会将最新的直播数据生成新的小文件，这样客户端只要不停的按顺序播放从服务器获取到的文件，就实现了直播。由此可见，基本上可以认为，HLS是以点播的技术方式实现直播。特点：由于数据通过HTTP协议传输，所以完全不用考虑防火墙或者代理的问题，而且分段文件的时长很短，客户端可以很快的选择和切换码率，以适应不同带宽条件下的播放。不过HLS的这种技术特点，决定了它的延迟一般总是会高于普通的流媒体直播协议。]]></content>
      <categories>
        <category>-基础</category>
      </categories>
      <tags>
        <tag>-音视频 -协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群下如何共享session]]></title>
    <url>%2F2019%2F09%2F20%2F%E9%9B%86%E7%BE%A4%E4%B8%8B%E5%A6%82%E4%BD%95%E5%85%B1%E4%BA%ABsession%2F</url>
    <content type="text"><![CDATA[Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中；session的用途：session的实现方法 Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式，Cookie是浏览器保存信息的一种方式，可以理解为一个文件，保存到客户端了啊，服务器可以通过响应浏览器的set-cookie的标头，得到Cookie的信息。你可以给这个文件设置一个期限，这个期限呢，不会因为浏览器的关闭而消失啊。其实大家应该对这个效果不陌生，很多购物网站都是这个做的，即使你没有买东西，他也记住了你的喜好，现在回来，会优先给你提交你喜欢的东西；用express 来实现cookies知识的学习https://blog.csdn.net/henni_719/article/details/53612499 cookies和session的区别 相关知识：cookie的存活期：默认为-1会话Cookie：当存活期为负数，把Cookie保存到浏览器上持久Cookie：当存活期为正数，把Cookie保存到文件中 有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。该方法的实现机制为：● 先判断当前的 Web 组件是否启用 Session，如果没有启用 Session，直接返回参数 url。● 再判断客户端浏览器是否支持 Cookie，如果支持 Cookie，直接返回参数 url；如果不支持 Cookie，就在参数 url 中加入 Session ID 信息，然后返回修改后的 url。 大部分手机浏览器不支持cookies 集群下共享session 的方法： 客户端cookie加密当用户登陆成功以后，把网站域名、用户名、密码、token、 session有效时间全部采用cookie的形式写入到客户端的cookie里面。如果用户从一台Web服务器跨越到另一台服务器的时候，我们的程序主动去检测客户端的cookie信息，进行判断，然后提供对应的服务，当然，如果cookie过期，或者无效，自然就不让用户继续服务了。 优点：简单、高效，也不会加大数据库的负担。缺点：cookie安全性较低，虽然它已经加了密，但是还是可以伪造的。但是如果客户端把cookie禁掉了的话，那么session就无从同步了，这样会给网站带来损失；cookies中数据不能太多，最好只有个用户id。 或者这个方法： 使用Nginx的ip_hash策略来做负载均衡ip_hash策略的原理：根据Ip做hash计算，同一个Ip的请求始终会定位到一台web服务器上。 用户—&gt; 浏览器 —-&gt; Nginx（ip_hash） —–&gt; n（多）台Web服务器 同一个用户（Ip）—&gt; 浏览器 —-&gt; Nginx（ip_hash） —–&gt; A Web服务器此时假如用户IP为 192.168.1.110 被hash到 A web服务器，那么用户的请求就一直会访问A web服务器了。优点： 配置简单，只需要在Nginx中配置好ip_hash 策略对应用无侵入性，应用不需要改任何ip_hash策略会很均匀的讲用户请求的ip分配到web服务器上，并且可以动态的水平扩展web服务器（只需在配置中加上服务器即可） 缺点:假如有一台Tomcat 宕机或者重启，那么这一台服务器上的用户的Session信息丢失，导致单点故障。 session replication，如tomcat自带session共享，主要是指集群环境下，多台应用服务器之间同步session，使session保持一致，对外透明。 如果其中一台服务器发生故障，根据负载均衡的原理，调度器会遍历寻找可用节点，分发请求，由于session已同步，故能保证用户的session信息不会丢失，会话复制,。优点：通过应用服务器配置即可，无需改动代码。缺点：性能随着服务器增加急剧下降，而且容易引起广播风暴；session数据需要序列化，影响性能。Session同步会有延迟，会影带宽受限于内存资源，在大用户量，高并发，高流量场景，会占用大量内存，不适合！ session复制的原理http://book.51cto.com/art/201202/319431.htm session复制的拓扑结构：点对点： 点对点复制的拓扑结构的优势：是不需要额外的进程和产品来避免单点失败，从而减少了配置和维持额外进程或产品的时间和费用的成本。劣势： 但这个拓扑结构的局限性就是它需要占用一定的内存空间，因为每个服务端都需要备份这个复制域里所有 session 的信息。假如一个 session 需要 10KB 的空间来存储信息，那么当 100 万个用户同时登陆到这个系统中时，每个应用服务器就需要花费 10GB 的内存空间来保存所有 session 的信息。 它的另一个局限性是每一个 session 信息的修改都会被复制到其他所有的应用服务器上，这极大地影响了整个系统的性能。 3.使用数据库保存session这种共享session的方式即将session信息存入数据库中，其它应用可以从数据库中查出session信息。优点：使用数据库来保存session,就算服务器宕机了也没事，session照样在。缺点：每次请求都需要对数据库进行读写，session的并发读写在数据库中完成，会加大数据库的IO，对数据库性能要求比较高。我们需要额外地实现session淘汰逻辑代码，即定时从数据库表中更新和删除session信息，增加了工作量。数据库读写速度较慢，不利于session的适时同步。 使用redis或memcache来保存session提供一个集群保存session共享信息，其他应用统统把自己的session信息存放到session集群服务器组。当应用系统需要session信息的时候直接到session群集服务器上读取。目前大多都是使用Memcache或redis来对Session进行存储。以这种方式来同步session，不会加大数据库的负担，并且安全性比用cookie大大的提高，把session放到内存里面，比从文件中读取要快很多。]]></content>
      <categories>
        <category>-后端知识</category>
      </categories>
      <tags>
        <tag>-服务器 -分布式 -session同步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈关系型数据库的外键]]></title>
    <url>%2F2019%2F09%2F20%2F%E8%B0%88%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%A4%96%E9%94%AE%2F</url>
    <content type="text"><![CDATA[外键多了会有很多维护问题吧？ 首先，外键本身是为了实现强一致性，所以如果需要正确性&gt;性能的话，还是建议使用外键，它可以让我们在数据库的层面保证数据的完整性和一致性。当然不用外键，你也可以在业务层进行实现。不过，这样做也同样存在一定的风险，因为这样，就会让业务逻辑会与数据具备一定的耦合性。也就是业务逻辑和数据必须同时修改。而且在工作中，业务层可能会经常发生变化。 当然，很多互联网的公司，尤其是超大型的数据应用场景，大量的插入，更新和删除在外键的约束下会降低性能，同时数据库在水平拆分和分库的情况下，数据库端也做不到执行外键约束。另外，在高并发的情况下，外键的存在也会造成额外的开销。因为每次更新数据，都需要检查另外一张表的数据，也容易造成死锁。所以在这种情况下，尤其是大型项目中后期，可以采用业务层来实现，取消外键提高效率。不过在SQL学习之初，包括在系统最初设计的时候，还是建议你采用规范的数据库设计，也就是采用外键来对数据表进行约束。因为这样可以建立一个强一致性，可靠性高的数据库结构，也不需要在业务层来实现过多的检查。当然在项目后期，业务量增大的情况下，你需要更多考虑到数据库性能问题，可以取消外键的约束，转移到业务层来实现。而且在大型互联网项目中，考虑到分库分表的情况，也会降低外键的使用。不过在SQL学习，以及项目早期，还是建议你使用外键。在项目后期，你可以分析有哪些外键造成了过多的性能消耗。一般遵循2/8原则，会有20%的外键造成80%的资源效率，你可以只把这20%的外键进行开放，采用业务层逻辑来进行实现，当然你需要保证业务层的实现没有错误。不同阶段，考虑的问题不同。当用户和业务量增大的时候，对于大型互联网应用，也会通过减少外键的使用，来减低死锁发生的概率，提高并发处理能力。]]></content>
      <categories>
        <category>-后端知识</category>
      </categories>
      <tags>
        <tag>-数据库 -外键 -实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[音视频编解码基础知识]]></title>
    <url>%2F2019%2F09%2F20%2F%E9%9F%B3%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[音视频编解码器有：OPUS 最新的协议，支持 口 耳模式 不支持RTMPAAC 用于直播系统多 支持RTMP 有高保真的效果Speex 支持回音消除，降噪算法 支持RTMPG.711 常用于固话 音频编解码频带和压缩性能对比图 AAC诞生的目的是为了取代MP3 mp3的规范是mpeg2 （mpeg2 有损压缩）mpeg4 规范中，AAC加入了SBR技术和PS技术 相较MPEG2音质更好，压缩比更高 AAC的常见规格 是AAC LC, AAC HE V1, AAC HE V2AAC HE V2 = AAC HE V1 + PSAAC HE V1 = AAC + SBR LC = low complexityhe = lc + sbr (spectral band replication 分频复用) 在lc中采样率固定的时候 低频采用的 次数 会很多，而高频采用次数会很少，不协调，he则降低低频采样次数增加高频采样次数 he v2 = aac lc + sbr +ps (双声道分别保存) 一个声道的数据完整保存，另一个声道只存差异数据 文件格式adif 格式 这种格式只能从头开始解码，音频各种信息都保存到文件头部adts格式 每一帧都有一个同步字，使得在流传输的时候可以从任何位置开始解码 在RTMP和FLV文件格式里都是使用adts AAC的编解码库 效率对比libfdk_aac &gt; ffmpeg_aac &gt; libfaac &gt; libvo_aacenc 手机是通过音视频采集模块将数据采集成 PCM/YUV数据，然后通过硬件编码器将它们压缩成 AAC（LC）/H264。 H264编码格式中B桢是向前向后参考帧，B帧越多，压缩比越高 ，但是在直播系统中，得等到下一帧来了才能解码，所以延迟会高 GOF = group of frame 一组帧 组帧头是SPS, PPS(他们两个被划入I帧)SPS 序列参数集， 存放帧信息，PPS 图像参数集 花屏的原因： GOF中的P帧丢失， 避免花屏，但一个GOF钟的P帧丢失就不显示这个GOF,但是会引起卡顿 视频编解码器x264 市面上用最多x265 压缩比更高，占用cpu更多open H264 性能低一些相对于x264支持SVC（视频分层传输，根据网络情况，选择发送层数）技术 SVC在移动端，很多硬件解码不支持，只能使用软编，使用svc可以适用各种带宽情况 H264压缩技术基础概念帧内预测压缩，对人眼不敏感的数据进行压缩帧间预测压缩 压缩帧与帧之间相同数据DCTcbac压缩，无损压缩，类似于哈夫曼编码将每一帧数据进行宏块划分，宏块内部再进行子块划分，对相似性高的 帧进行帧规划，形成帧分组 帧间预测压缩 过程组内宏块查找 -》 运动估算 -》 运动矢量和补偿压缩 -》得到运动矢量数据 + 基础背景数据 帧内预测压缩过程对帧内的所有宏块进行选择，选择一种帧内预测模式（共九种模式）对已经进行预测的图片和原图进行对比 计算出帧内预测残缺值，得到残缺值图，如下右图 黑色部分最后把预测的图片和残缺值进行保存压缩。解压的时候 把 预测图片和 残缺值进行相加即可得出原图 DCT压缩把宏块进行量化，然后丢入DCT压缩算法，得到量化结果 VLC压缩 是MPEG2 规范中的 （类似哈夫曼编码的） 但是是有损的H264规范中 使用的是CABAC 压缩，还增加了 上下文适用的压缩规则 无损压缩 H264的结构图H264编码分层NAL 层以太网最多1500字节 为传输单元 而H264的帧大于1500字节。所以需要进行拆包，NAL层就进行拆包组包工作VCL video coding layer 视频数据编码层 就是如上讲是视频压缩算法编码算法等 码流的概念：SODB string of data bits 原始码流，长度不一定是8的倍速 由VCL产生RBSP 在SODB 后补位 补足为8的倍速EBSP 在每个帧的起始位 加入特殊标识 NALU 就是在EBSP上 加一个字节的NAL起始头形成NAL单元H264中把一个帧分为 n个切片， 一帧至少有一个切片（因为在网络传输的时候，可能会把一帧拆开进行传输，就按照切片分）nal单元 = nalu 头 + 一个切片 （切片头 + 切片数据）切片和宏块的关系 起始码是起始标记 0x00 0x01 YUV 颜色编码方法 Y 明亮度 UV 表示色度YUV的组织形式yuv 存储格式]]></content>
      <categories>
        <category>-音视频</category>
      </categories>
      <tags>
        <tag>-编解码 -基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务解决方案]]></title>
    <url>%2F2019%2F09%2F20%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[当我们的单个数据库的性能产生瓶颈的时候，我们可能会对数据库进行分区，这里所说的分区指的是物理分区，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的ACID已经不能适应这种情况。于是使用新的理论来保证分布式事务的一致性。 CAP定理一致性(Consistency) ：保证服务中所有的节点保存的数据都是一致的。 可用性(Availability) ： 读写操作在单台服务器出问题后，在其他服务器上依然能够完成读写操作重点在于：某个读写操作在出问题的机器上不能读写了，但是在其他机器可以完成 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成。（分布式系统遇到网络分区故障的时候，某个分区故障了，但是其他的分区还可用保证一致性和可用性。这样丧失了分区容错性。且剩下的服务不会因为坏掉的部分节点而丧失一致性和可用性）重点在于：部分服务器因网络问题，业务依然能够继续运行 三种特性只能同时满足两个。可以使用zookeeper和euraka架构的例子。zookeeper使用leader选举（这个制度，会导致这个服务组件无法对外提供服务丧失了可用性）制度保证了一致性和分区容错性，服务实例自动切换euraka节点（由于原euraka节点挂掉了，但euraka上面的数据还没同步到其他节点上，此时即使服务实例切换了节点，但是在一致性上就没有得到保障，虽然服务实例通过心跳包机制或者ack机制，发现实例挂掉了，再重新注册，保障了最终一致性。）重新注册保证了可用性和分区容错性。 在分布式系统中，我们往往追求的是可用性，它的重要程序比一致性要高，那么如何实现高可用性呢？ 前人已经给我们提出来了另外一个理论，就是BASE理论 Basically Available（基本可用）Soft state（软状态）Eventually consistent（最终一致性） BASE理论是对CAP中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性 常见的处理方式有：基于XA协议的两阶段提交方案，TCC方案，基于消息的最终一致性方案 基于XA协议的两阶段提交方案：事务调度器和分布在各地的资源管理器通过两阶段提交方式来完成一个全局事务。第一阶段是表决阶段，所有参与者都将本事务能否成功的信息反馈发给协调者；第二阶段是执行阶段，协调者根据所有参与者的反馈，通知所有参与者，步调一致地在所有分支上提交或者回滚。虽然两阶段提交方案应用非常广泛，几乎所有商业OLTP数据库都支持XA协议。存在的问题有：2.1 同步阻塞 所有事务参与者在等待其它参与者响应的时候都处于同步阻塞状态，无法进行其它操作。2.2 单点问题 协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响。特别是在阶段二发生故障，所有参与者会一直等待状态，无法完成其它操作。2.3 数据不一致 在阶段二，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。2.4 太过保守 任意一个节点失败就会导致整个事务失败，没有完善的容错机制。 TCC方案：TCC方案在电商、金融领域落地较多。这种方案不借助MQ。它类似于关系型数据库事务的三种操作：DML、Commit和Rollback，其将整个业务逻辑的每个分支显式的分成了Try、Confirm、Cancel三个操作。Try部分完成业务的准备工作，confirm部分完成业务的提交，cancel部分完成事务的回滚。事务开始时，业务应用会向事务协调器注册启动事务。之后业务应用会调用所有服务的try接口，完成一阶段准备。之后业务应用收到try接口的返回情况，然后决定给事务调度器调用confirm接口或者cancel接口。如果接口调用失败，会进行重试。 TCC方案让应用自己定义数据库操作的粒度（粒度小主要体现在，相较于2pc方式，try阶段相较于2pc的预备阶段，它提前锁定资源的，然后再执行后续操作，如果confirm执行超时或者失败即可回退，而2pc的预备阶段是先把需要的资源锁住，如果资源不够就返回失败，在执行提交事务的过程中如果某一个操作执行失败，那么由于没有回退操作，那么其他资源就会一直被锁定）。由于Confirm和Cancel操作可能被重复调用，故要求Confirm和Cancel两个接口必须是幂等的。缺点： ● 对应用的侵入性强。业务逻辑的每个分支都需要实现try、confirm、cancel三个操作，应用侵入性较强，改造成本高。 ● 实现难度较大。需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，confirm和cancel接口必须实现幂等。 基于本地消息表的最终一致性方案：核心思想是将分布式事务拆分成本地事务进行处理。消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送到消息队列或者消息消费放失败，会进行重试发送。消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，就会发一个消息返回给消息生产方，修改消息表中的状态。如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。这种方案遵循BASE理论，采用的是最终一致性，笔者认为是这几种方案里面比较适合实际业务场景的，即不会出现像2PC那样复杂的实现(当调用链很长的时候，2PC的可用性是非常低的)，也不会像TCC那样可能出现确认或者回滚不了的情况。优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。 基于消息队列的事务方式方案：有一些第三方的MQ是支持事务消息的，比如RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交，但是市面上一些主流的MQ都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持。操作流程如下：1.发送prepare消息，该消息对Consumer不可见2.执行本地事务3.若本地事务执行成功，则向MQ提交消息确认发送指令；若本地事务执行失败，则向MQ发送取消指令4.若MQ长时间未收到确认发送或取消发送的指令，则向业务系统询问本地事务状态，并做补偿处理。优点： 实现了最终一致性，不需要依赖本地数据库事务。也不依赖分布式事务。缺点： 实现难度大，主流MQ不支持。]]></content>
      <categories>
        <category>-后端知识</category>
      </categories>
      <tags>
        <tag>-服务器 -分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式，集群，微服务的理解]]></title>
    <url>%2F2019%2F09%2F20%2F%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%8C%E9%9B%86%E7%BE%A4%EF%BC%8C%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[分布式：一个业务分拆多个子业务，部署在不同的服务器上集群：同一个业务，部署在多个服务器上微服务：是一种架构风格，一个大型复杂软件应用由一个或多个微服务组成。系统中的各个微服务可被独立部署，微服务之间是松耦合的。每个微服务仅关注于完成一件任务并很好地完成该任务。在所有情况下，每个任务代表着一个小的业务能力。 分布式是以缩短单个任务的执行时间来提升效率的，而集群则是通过提高单位时间内执行的任务数来提升效率。采用集群方案，同样提供 10 台服务器，每台服务器都能独立处理这个任务。假设有 10 个任务同时到达，10 个服务器将同时工作，1 小时后，10 个任务同时完成，这样，整身来看，还是 1 小时内完成一个任务！ 采用微服务方案，为了不因为某个模块的升级和BUG影响现有的系统业务。微服务与分布式的细微差别是，微服务的应用不一定是分散在多个服务器上，他也可以是同一个服务器。 需注意的地方：分布式模式下需要做好事务管理和单点故障的问题。集群模式是不同服务器部署同一套服务对外访问，实现服务的负载均衡。注：集群模式需要做好session共享，确保在不同服务器切换的过程中不会因为没有获取到session而中止退出服务。一般配置Nginx*的负载容器实现：静态资源缓存、Session共享可以附带实现，Nginx支持五万个并发量。微服务模式下：1）单体应用拆分为分布式系统后，进程间的通讯机制和故障处理措施变的更加复杂。2）系统微服务化后，一个看似简单的功能，内部可能需要调用多个服务并操作多个数据库实现，服务调用的分布式事务问题变的非常突出。3）微服务数量众多，其测试、部署、监控等都变的更加困难。但是实际中dubbo可以支持多种通讯协议，springcloud可以非常好的支持restful调用。对于第三个问题，随着docker、devops技术的发展以及各公有云paas平台自动化运维工具的推出，微服务的测试、部署与运维会变得越来越容易。]]></content>
  </entry>
  <entry>
    <title><![CDATA[test_pic]]></title>
    <url>%2F2019%2F09%2F20%2Ftest-pic%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[test to use blog]]></title>
    <url>%2F2019%2F09%2F19%2Ftest-to-use-blog%2F</url>
    <content type="text"></content>
  </entry>
</search>
