<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[音视频编解码基础知识]]></title>
    <url>%2F2019%2F09%2F20%2F%E9%9F%B3%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[音视频编解码器有：OPUS 最新的协议，支持 口 耳模式 不支持RTMPAAC 用于直播系统多 支持RTMP 有高保真的效果Speex 支持回音消除，降噪算法 支持RTMPG.711 常用于固话 音频编解码频带和压缩性能对比图 AAC诞生的目的是为了取代MP3 mp3的规范是mpeg2 （mpeg2 有损压缩）mpeg4 规范中，AAC加入了SBR技术和PS技术 相较MPEG2音质更好，压缩比更高 AAC的常见规格 是AAC LC, AAC HE V1, AAC HE V2AAC HE V2 = AAC HE V1 + PSAAC HE V1 = AAC + SBR LC = low complexityhe = lc + sbr (spectral band replication 分频复用) 在lc中采样率固定的时候 低频采用的 次数 会很多，而高频采用次数会很少，不协调，he则降低低频采样次数增加高频采样次数 he v2 = aac lc + sbr +ps (双声道分别保存) 一个声道的数据完整保存，另一个声道只存差异数据 文件格式adif 格式 这种格式只能从头开始解码，音频各种信息都保存到文件头部adts格式 每一帧都有一个同步字，使得在流传输的时候可以从任何位置开始解码 在RTMP和FLV文件格式里都是使用adts AAC的编解码库 效率对比libfdk_aac &gt; ffmpeg_aac &gt; libfaac &gt; libvo_aacenc 手机是通过音视频采集模块将数据采集成 PCM/YUV数据，然后通过硬件编码器将它们压缩成 AAC（LC）/H264。 H264编码格式中B桢是向前向后参考帧，B帧越多，压缩比越高 ，但是在直播系统中，得等到下一帧来了才能解码，所以延迟会高 GOF = group of frame 一组帧 组帧头是SPS, PPS(他们两个被划入I帧)SPS 序列参数集， 存放帧信息，PPS 图像参数集 花屏的原因： GOF中的P帧丢失， 避免花屏，但一个GOF钟的P帧丢失就不显示这个GOF,但是会引起卡顿 视频编解码器x264 市面上用最多x265 压缩比更高，占用cpu更多open H264 性能低一些相对于x264支持SVC（视频分层传输，根据网络情况，选择发送层数）技术 SVC在移动端，很多硬件解码不支持，只能使用软编，使用svc可以适用各种带宽情况 H264压缩技术基础概念帧内预测压缩，对人眼不敏感的数据进行压缩帧间预测压缩 压缩帧与帧之间相同数据DCTcbac压缩，无损压缩，类似于哈夫曼编码将每一帧数据进行宏块划分，宏块内部再进行子块划分，对相似性高的 帧进行帧规划，形成帧分组 帧间预测压缩 过程组内宏块查找 -》 运动估算 -》 运动矢量和补偿压缩 -》得到运动矢量数据 + 基础背景数据 帧内预测压缩过程对帧内的所有宏块进行选择，选择一种帧内预测模式（共九种模式）对已经进行预测的图片和原图进行对比 计算出帧内预测残缺值，得到残缺值图，如下右图 黑色部分最后把预测的图片和残缺值进行保存压缩。解压的时候 把 预测图片和 残缺值进行相加即可得出原图 DCT压缩把宏块进行量化，然后丢入DCT压缩算法，得到量化结果 VLC压缩 是MPEG2 规范中的 （类似哈夫曼编码的） 但是是有损的H264规范中 使用的是CABAC 压缩，还增加了 上下文适用的压缩规则 无损压缩 H264的结构图H264编码分层NAL 层以太网最多1500字节 为传输单元 而H264的帧大于1500字节。所以需要进行拆包，NAL层就进行拆包组包工作VCL video coding layer 视频数据编码层 就是如上讲是视频压缩算法编码算法等 码流的概念：SODB string of data bits 原始码流，长度不一定是8的倍速 由VCL产生RBSP 在SODB 后补位 补足为8的倍速EBSP 在每个帧的起始位 加入特殊标识 NALU 就是在EBSP上 加一个字节的NAL起始头形成NAL单元H264中把一个帧分为 n个切片， 一帧至少有一个切片（因为在网络传输的时候，可能会把一帧拆开进行传输，就按照切片分）nal单元 = nalu 头 + 一个切片 （切片头 + 切片数据）切片和宏块的关系 起始码是起始标记 0x00 0x01 YUV 颜色编码方法 Y 明亮度 UV 表示色度YUV的组织形式yuv 存储格式]]></content>
  </entry>
  <entry>
    <title><![CDATA[分布式事务解决方案]]></title>
    <url>%2F2019%2F09%2F20%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[当我们的单个数据库的性能产生瓶颈的时候，我们可能会对数据库进行分区，这里所说的分区指的是物理分区，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的ACID已经不能适应这种情况。于是使用新的理论来保证分布式事务的一致性。 CAP定理一致性(Consistency) ：保证服务中所有的节点保存的数据都是一致的。 可用性(Availability) ： 读写操作在单台服务器出问题后，在其他服务器上依然能够完成读写操作重点在于：某个读写操作在出问题的机器上不能读写了，但是在其他机器可以完成 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成。（分布式系统遇到网络分区故障的时候，某个分区故障了，但是其他的分区还可用保证一致性和可用性。这样丧失了分区容错性。且剩下的服务不会因为坏掉的部分节点而丧失一致性和可用性）重点在于：部分服务器因网络问题，业务依然能够继续运行 三种特性只能同时满足两个。可以使用zookeeper和euraka架构的例子。zookeeper使用leader选举（这个制度，会导致这个服务组件无法对外提供服务丧失了可用性）制度保证了一致性和分区容错性，服务实例自动切换euraka节点（由于原euraka节点挂掉了，但euraka上面的数据还没同步到其他节点上，此时即使服务实例切换了节点，但是在一致性上就没有得到保障，虽然服务实例通过心跳包机制或者ack机制，发现实例挂掉了，再重新注册，保障了最终一致性。）重新注册保证了可用性和分区容错性。 在分布式系统中，我们往往追求的是可用性，它的重要程序比一致性要高，那么如何实现高可用性呢？ 前人已经给我们提出来了另外一个理论，就是BASE理论 Basically Available（基本可用）Soft state（软状态）Eventually consistent（最终一致性） BASE理论是对CAP中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性 常见的处理方式有：基于XA协议的两阶段提交方案，TCC方案，基于消息的最终一致性方案 基于XA协议的两阶段提交方案：事务调度器和分布在各地的资源管理器通过两阶段提交方式来完成一个全局事务。第一阶段是表决阶段，所有参与者都将本事务能否成功的信息反馈发给协调者；第二阶段是执行阶段，协调者根据所有参与者的反馈，通知所有参与者，步调一致地在所有分支上提交或者回滚。虽然两阶段提交方案应用非常广泛，几乎所有商业OLTP数据库都支持XA协议。存在的问题有：2.1 同步阻塞 所有事务参与者在等待其它参与者响应的时候都处于同步阻塞状态，无法进行其它操作。2.2 单点问题 协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响。特别是在阶段二发生故障，所有参与者会一直等待状态，无法完成其它操作。2.3 数据不一致 在阶段二，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。2.4 太过保守 任意一个节点失败就会导致整个事务失败，没有完善的容错机制。 TCC方案：TCC方案在电商、金融领域落地较多。这种方案不借助MQ。它类似于关系型数据库事务的三种操作：DML、Commit和Rollback，其将整个业务逻辑的每个分支显式的分成了Try、Confirm、Cancel三个操作。Try部分完成业务的准备工作，confirm部分完成业务的提交，cancel部分完成事务的回滚。事务开始时，业务应用会向事务协调器注册启动事务。之后业务应用会调用所有服务的try接口，完成一阶段准备。之后业务应用收到try接口的返回情况，然后决定给事务调度器调用confirm接口或者cancel接口。如果接口调用失败，会进行重试。 TCC方案让应用自己定义数据库操作的粒度（粒度小主要体现在，相较于2pc方式，try阶段相较于2pc的预备阶段，它提前锁定资源的，然后再执行后续操作，如果confirm执行超时或者失败即可回退，而2pc的预备阶段是先把需要的资源锁住，如果资源不够就返回失败，在执行提交事务的过程中如果某一个操作执行失败，那么由于没有回退操作，那么其他资源就会一直被锁定）。由于Confirm和Cancel操作可能被重复调用，故要求Confirm和Cancel两个接口必须是幂等的。缺点： ● 对应用的侵入性强。业务逻辑的每个分支都需要实现try、confirm、cancel三个操作，应用侵入性较强，改造成本高。 ● 实现难度较大。需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，confirm和cancel接口必须实现幂等。 基于本地消息表的最终一致性方案：核心思想是将分布式事务拆分成本地事务进行处理。消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送到消息队列或者消息消费放失败，会进行重试发送。消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，就会发一个消息返回给消息生产方，修改消息表中的状态。如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。这种方案遵循BASE理论，采用的是最终一致性，笔者认为是这几种方案里面比较适合实际业务场景的，即不会出现像2PC那样复杂的实现(当调用链很长的时候，2PC的可用性是非常低的)，也不会像TCC那样可能出现确认或者回滚不了的情况。优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。 基于消息队列的事务方式方案：有一些第三方的MQ是支持事务消息的，比如RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交，但是市面上一些主流的MQ都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持。操作流程如下：1.发送prepare消息，该消息对Consumer不可见2.执行本地事务3.若本地事务执行成功，则向MQ提交消息确认发送指令；若本地事务执行失败，则向MQ发送取消指令4.若MQ长时间未收到确认发送或取消发送的指令，则向业务系统询问本地事务状态，并做补偿处理。优点： 实现了最终一致性，不需要依赖本地数据库事务。也不依赖分布式事务。缺点： 实现难度大，主流MQ不支持。]]></content>
      <categories>
        <category>-后端知识</category>
      </categories>
      <tags>
        <tag>-服务器 -分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式，集群，微服务的理解]]></title>
    <url>%2F2019%2F09%2F20%2F%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%8C%E9%9B%86%E7%BE%A4%EF%BC%8C%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[分布式：一个业务分拆多个子业务，部署在不同的服务器上集群：同一个业务，部署在多个服务器上微服务：是一种架构风格，一个大型复杂软件应用由一个或多个微服务组成。系统中的各个微服务可被独立部署，微服务之间是松耦合的。每个微服务仅关注于完成一件任务并很好地完成该任务。在所有情况下，每个任务代表着一个小的业务能力。 分布式是以缩短单个任务的执行时间来提升效率的，而集群则是通过提高单位时间内执行的任务数来提升效率。采用集群方案，同样提供 10 台服务器，每台服务器都能独立处理这个任务。假设有 10 个任务同时到达，10 个服务器将同时工作，1 小时后，10 个任务同时完成，这样，整身来看，还是 1 小时内完成一个任务！ 采用微服务方案，为了不因为某个模块的升级和BUG影响现有的系统业务。微服务与分布式的细微差别是，微服务的应用不一定是分散在多个服务器上，他也可以是同一个服务器。 需注意的地方：分布式模式下需要做好事务管理和单点故障的问题。集群模式是不同服务器部署同一套服务对外访问，实现服务的负载均衡。注：集群模式需要做好session共享，确保在不同服务器切换的过程中不会因为没有获取到session而中止退出服务。一般配置Nginx*的负载容器实现：静态资源缓存、Session共享可以附带实现，Nginx支持五万个并发量。微服务模式下：1）单体应用拆分为分布式系统后，进程间的通讯机制和故障处理措施变的更加复杂。2）系统微服务化后，一个看似简单的功能，内部可能需要调用多个服务并操作多个数据库实现，服务调用的分布式事务问题变的非常突出。3）微服务数量众多，其测试、部署、监控等都变的更加困难。但是实际中dubbo可以支持多种通讯协议，springcloud可以非常好的支持restful调用。对于第三个问题，随着docker、devops技术的发展以及各公有云paas平台自动化运维工具的推出，微服务的测试、部署与运维会变得越来越容易。]]></content>
  </entry>
  <entry>
    <title><![CDATA[test_pic]]></title>
    <url>%2F2019%2F09%2F20%2Ftest-pic%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[test to use blog]]></title>
    <url>%2F2019%2F09%2F19%2Ftest-to-use-blog%2F</url>
    <content type="text"></content>
  </entry>
</search>
